{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Title\n",
    "\n",
    "Project website: https://docs.google.com/document/d/1GkK28wOyjTPZEZuOumKPU0z1NzVT_9sxETjPOcLPVYo/edit?tab=t.0#heading=h.qifoo7co6qtd\n",
    "\n",
    "Professor website: https://malchiodi.di.unimi.it/teaching/AMD-DSE/2024-25/en"
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-30T13:42:30.384654Z",
     "start_time": "2025-04-30T13:42:27.139401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Install required packages from the requirement.txt file if not already installed\n",
    "!pip install -r requirements.txt"
   ],
   "id": "4b98cbd80fe8cd48",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in c:\\users\\maidi\\miniconda3\\envs\\python312bigdata\\lib\\site-packages (from -r requirements.txt (line 1)) (1.7.4.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\maidi\\miniconda3\\envs\\python312bigdata\\lib\\site-packages (from -r requirements.txt (line 2)) (1.6.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\maidi\\miniconda3\\envs\\python312bigdata\\lib\\site-packages (from -r requirements.txt (line 3)) (2.0.1)\n",
      "Requirement already satisfied: pyspark in c:\\users\\maidi\\miniconda3\\envs\\python312bigdata\\lib\\site-packages (from -r requirements.txt (line 4)) (3.1.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\maidi\\miniconda3\\envs\\python312bigdata\\lib\\site-packages (from -r requirements.txt (line 5)) (3.9.1)\n",
      "Requirement already satisfied: bleach in c:\\users\\maidi\\miniconda3\\envs\\python312bigdata\\lib\\site-packages (from kaggle->-r requirements.txt (line 1)) (6.2.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in c:\\users\\maidi\\miniconda3\\envs\\python312bigdata\\lib\\site-packages (from kaggle->-r requirements.txt (line 1)) (2025.4.26)\n",
      "Requirement already satisfied: charset-normalizer in c:\\users\\maidi\\miniconda3\\envs\\python312bigdata\\lib\\site-packages (from kaggle->-r requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: idna in c:\\users\\maidi\\miniconda3\\envs\\python312bigdata\\lib\\site-packages (from kaggle->-r requirements.txt (line 1)) (3.7)\n",
      "Requirement already satisfied: protobuf in c:\\users\\maidi\\miniconda3\\envs\\python312bigdata\\lib\\site-packages (from kaggle->-r requirements.txt (line 1)) (6.30.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\maidi\\miniconda3\\envs\\python312bigdata\\lib\\site-packages (from kaggle->-r requirements.txt (line 1)) (2.9.0.post0)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\maidi\\miniconda3\\envs\\python312bigdata\\lib\\site-packages (from kaggle->-r requirements.txt (line 1)) (8.0.4)\n",
      "Requirement already satisfied: requests in c:\\users\\maidi\\miniconda3\\envs\\python312bigdata\\lib\\site-packages (from kaggle->-r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in c:\\users\\maidi\\miniconda3\\envs\\python312bigdata\\lib\\site-packages (from kaggle->-r requirements.txt (line 1)) (75.8.0)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\maidi\\miniconda3\\envs\\python312bigdata\\lib\\site-packages (from kaggle->-r requirements.txt (line 1)) (1.17.0)\n",
      "Requirement already satisfied: text-unidecode in c:\\users\\maidi\\miniconda3\\envs\\python312bigdata\\lib\\site-packages (from kaggle->-r requirements.txt (line 1)) (1.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\maidi\\miniconda3\\envs\\python312bigdata\\lib\\site-packages (from kaggle->-r requirements.txt (line 1)) (4.67.1)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in c:\\users\\maidi\\miniconda3\\envs\\python312bigdata\\lib\\site-packages (from kaggle->-r requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\maidi\\miniconda3\\envs\\python312bigdata\\lib\\site-packages (from kaggle->-r requirements.txt (line 1)) (0.5.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\maidi\\miniconda3\\envs\\python312bigdata\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 2)) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\maidi\\miniconda3\\envs\\python312bigdata\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 2)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\maidi\\miniconda3\\envs\\python312bigdata\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 2)) (3.6.0)\n",
      "Requirement already satisfied: py4j==0.10.9 in c:\\users\\maidi\\miniconda3\\envs\\python312bigdata\\lib\\site-packages (from pyspark->-r requirements.txt (line 4)) (0.10.9)\n",
      "Requirement already satisfied: click in c:\\users\\maidi\\miniconda3\\envs\\python312bigdata\\lib\\site-packages (from nltk->-r requirements.txt (line 5)) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\maidi\\miniconda3\\envs\\python312bigdata\\lib\\site-packages (from nltk->-r requirements.txt (line 5)) (2024.11.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\maidi\\miniconda3\\envs\\python312bigdata\\lib\\site-packages (from click->nltk->-r requirements.txt (line 5)) (0.4.6)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-04-30T13:42:30.385647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Set environment variables for Kaggle API\n",
    "os.environ['KAGGLE_USERNAME'] = \"ilchurch\"\n",
    "os.environ['KAGGLE_KEY'] = \"5449548f4b6c8556c8f48f4a31b4ea6e\"\n",
    "\n",
    "# Ensure kaggle is installed\n",
    "try:\n",
    "    import kaggle\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"kaggle\"])\n",
    "    import kaggle\n",
    "\n",
    "# Import and authenticate using KaggleApi\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# Download and unzip the dataset\n",
    "api.dataset_download_files('mohamedbakhet/amazon-books-reviews', path='data', unzip=True)"
   ],
   "id": "9edc57113f58ee11",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/mohamedbakhet/amazon-books-reviews\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# amazon_books_data\n",
    "# The dataset contains the following files:\n",
    "# - books_data.csv: \n",
    "# - Books_rating.csv:"
   ],
   "id": "aca7278898d62aff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the dataset with spark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"JaccardSimilarity\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "books_rating = spark.read.csv(\"data/Books_rating.csv\", header=True, inferSchema=True)"
   ],
   "id": "faa012f3baac4263",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Display the first few rows of the books_rating dataset\n",
    "books_rating.show(5)"
   ],
   "id": "6f55fd13c5c4a70c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Create a subsample of the dataset for performance reasons\n",
    "I will use a sample of the dataset as a test case. My local and Google Collab computers are not powerful enough to handle the full dataset. I will use *sample* as a parameter can it can be changed to the desired sample size."
   ],
   "id": "32f883874724060"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "sample = 0.01 # 1% of the dataset",
   "id": "53024733b148bdb4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "books_rating_sample = books_rating.sample(fraction=sample, seed=42)\n",
    "books_rating_sample.show(5)\n",
    "books_rating_sample.count()"
   ],
   "id": "c1caef794a13d7d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Checking for null values in the 'review/text' column\n",
    "Since we are using text data, we need to check for null values in the 'review/text' column in order to take a decision on how to handle them."
   ],
   "id": "dcb6e82c7d972535"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Check for null values in the 'review/text' column\n",
    "null_rows_df = books_rating.filter(\n",
    "    books_rating['review/text'].isNull() | (books_rating['review/text'].rlike('^\\\\s*$'))\n",
    ")\n",
    "\n",
    "null_rows_df.show(30)"
   ],
   "id": "dcf56755a2c3a6d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Since the 'review/text' null values are more or less duplicates. \n",
    "\n",
    "They share the same columns, besides the identifiers and the Title. Although the Title is not the same, with a qualitative check we can see they are about the same book."
   ],
   "id": "13b84fb5ae57aa62"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import col\n",
    "filtered_df = books_rating.filter(\n",
    "    col(\"Title\").contains(\"Lord of the Rings\")\n",
    ").select(\"Title\", \"review/text\").limit(100)\n",
    "\n",
    "# Show the result\n",
    "filtered_df.show(100, truncate=False)"
   ],
   "id": "5885180066a66507",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "There are many others Lord of The Rings reviews. I think we can't discard the NA values and it's better to keep them. We can use also the Title as similarity check.",
   "id": "6dcd6513675d7e10"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "a2cdb61db5f854cf"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Check for null values in the 'review/text' column\n",
    "null_rows_df_sample = books_rating_sample.filter(\n",
    "    books_rating_sample['review/text'].isNull() | (books_rating_sample['review/text'].rlike('^\\\\s*$'))\n",
    ")\n",
    "\n",
    "null_rows_df_sample.show(30)"
   ],
   "id": "45b44dde088bb228",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The is a null value in the subsample",
   "id": "fbdccfaafc82187e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Working with the subsample",
   "id": "533cd4951d48f00d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Jaccard Similarity Approach\n",
    "Since the dataset is too big, I will use a subsample of the dataset to test the Jaccard Similarity approach, for test purposes. I will then try top scale it to the full dataset.\n",
    "In addition, I will use the MapReduce approach to parallelize the computation of the Jaccard Similarity."
   ],
   "id": "1072c0152b9ecab2"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import ArrayType, StringType"
   ],
   "id": "5885315f727b57eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "tokenizer = RegexTokenizer(inputCol=\"review/text\", outputCol=\"reviews/tokens\", pattern=\"\\\\W\") # \\\\W is a regex pattern that matches any non-word character",
   "id": "c6be9ffd0f620cb9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "It tokenizes the text into words. I am trying this approach because I believe it will be faster then some neural network based approach, since this use sparks. It will have some limitations, since it does not take in account of compound words, but I think it will be good for the first approach.",
   "id": "d1275a324da74d22"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "stopwords_remover = StopWordsRemover(inputCol=\"reviews/tokens\", outputCol=\"tokens/nostopwords\")",
   "id": "1fca258eea1e93f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "StopWordsRemover() uses a predefined list of stop words in English. It removes common words that do not carry much meaning, such as \"the\", \"is\", \"in\", etc. This is important, since the Jaccard Similarity is based on the number of common words between two texts. If we don't remove the stop words, the Jaccard Similarity will be biased by the number of stop words in the texts, which are not relevant for the meaning of the texts.",
   "id": "87c07acc08025c2c"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_numbers(tokens):\n",
    "    return [token for token in tokens if not token.isdigit()]"
   ],
   "id": "4707a6f993b28803",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Numbers are often not relevant for the meaning of a review. In our dataset, the score is stored in a separated column, so we can remove the numbers from the text.",
   "id": "593cab392a6b24df"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "remove_numbers_udf = udf(remove_numbers, ArrayType(StringType())) # Specify the return type as ArrayType(StringType), meaning an array/list of strings",
   "id": "5a62598dcf6f25a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Sparks does not work as a Python object, so we need UDF (user defined function) to use it in the pipeline.",
   "id": "c55d3aa47c162dab"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_books_rating_sample = tokenizer.transform(books_rating_sample) # Apply the tokenizer to the dataset\n",
    "tokenized_reviews_nostopwords_sample = stopwords_remover.transform(tokenized_books_rating_sample) # Remove the stop words from the tokenized dataset\n",
    "tokenized_rw_nsw_nn_sample = tokenized_reviews_nostopwords_sample.withColumn(\"tokens/nostopwords/nonumbers\", remove_numbers_udf(col(\"tokens/nostopwords\"))) # Remove the numbers from the dataset"
   ],
   "id": "ddaa3e8ad4c3282",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "tokenized_rw_nsw_nn_sample.show(10)",
   "id": "337bce825f5607c6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (python312bigdata)",
   "language": "python",
   "name": "python312bigdata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
