{
  "cells": [
    {
      "metadata": {
        "id": "8a77807f92f26ee"
      },
      "cell_type": "markdown",
      "source": [
        "# Title\n",
        "\n",
        "Project website: https://docs.google.com/document/d/1GkK28wOyjTPZEZuOumKPU0z1NzVT_9sxETjPOcLPVYo/edit?tab=t.0#heading=h.qifoo7co6qtd\n",
        "\n",
        "Professor website: https://malchiodi.di.unimi.it/teaching/AMD-DSE/2024-25/en"
      ],
      "id": "8a77807f92f26ee"
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/chiesastefano/massiveData\n",
        "import os\n",
        "os.chdir(\"massiveData\")"
      ],
      "metadata": {
        "id": "RIRey329LtTM",
        "outputId": "d6775a4d-dccd-4144-b951-89ae5ad5f53d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "RIRey329LtTM",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.81)] [\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.81)] [\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:5 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "python3.10 is already the newest version (3.10.12-1~22.04.9).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n",
            "fatal: destination path 'massiveData' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Update alternatives to set Python 3.10 as the default\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 1"
      ],
      "metadata": {
        "id": "72LRJ7Sc3sBn"
      },
      "id": "72LRJ7Sc3sBn",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-30T14:37:00.602212Z",
          "start_time": "2025-04-30T14:36:57.821578Z"
        },
        "id": "4b98cbd80fe8cd48",
        "outputId": "0883d01a-725b-4091-f215-29b0314d9ae2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        }
      },
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import sys\n",
        "# Install required packages from the requirement.txt file if not already installed\n",
        "!pip install -r requirements.txt trigger a runtime restart\n",
        "exit()"
      ],
      "id": "4b98cbd80fe8cd48",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (1.7.4.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (1.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (2.0.2)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (3.5.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (3.9.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle->-r requirements.txt (line 1)) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle->-r requirements.txt (line 1)) (2025.4.26)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle->-r requirements.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle->-r requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle->-r requirements.txt (line 1)) (5.29.4)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle->-r requirements.txt (line 1)) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle->-r requirements.txt (line 1)) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle->-r requirements.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle->-r requirements.txt (line 1)) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle->-r requirements.txt (line 1)) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle->-r requirements.txt (line 1)) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle->-r requirements.txt (line 1)) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle->-r requirements.txt (line 1)) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle->-r requirements.txt (line 1)) (0.5.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 2)) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 2)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 2)) (3.6.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark->-r requirements.txt (line 4)) (0.10.9.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->-r requirements.txt (line 5)) (8.1.8)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->-r requirements.txt (line 5)) (2024.11.6)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileExistsError",
          "evalue": "[Errno 17] File exists: '/usr/bin/python3.10' -> '/usr/bin/python'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-89a95719a9d3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/usr/bin/python3.10'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/usr/bin/python'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Exit to trigger a runtime restart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/usr/bin/python3.10' -> '/usr/bin/python'"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-30T14:39:03.657999Z",
          "start_time": "2025-04-30T14:37:00.603208Z"
        },
        "id": "9edc57113f58ee11"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Set environment variables for Kaggle API\n",
        "os.environ['KAGGLE_USERNAME'] = \"ilchurch\"\n",
        "os.environ['KAGGLE_KEY'] = \"5449548f4b6c8556c8f48f4a31b4ea6e\"\n",
        "\n",
        "# Ensure kaggle is installed\n",
        "try:\n",
        "    import kaggle\n",
        "except ImportError:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"kaggle\"])\n",
        "    import kaggle\n",
        "\n",
        "# Import and authenticate using KaggleApi\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "api = KaggleApi()\n",
        "api.authenticate()\n",
        "\n",
        "# Check if the dataset file already exists\n",
        "if not os.path.exists('data/Books_rating.csv'):\n",
        "    api.dataset_download_files('mohamedbakhet/amazon-books-reviews', path='data', unzip=True)\n",
        "else:\n",
        "    print(\"Dataset already exists.\")"
      ],
      "id": "9edc57113f58ee11",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-30T14:39:09.310108Z",
          "start_time": "2025-04-30T14:39:03.658511Z"
        },
        "id": "faa012f3baac4263"
      },
      "cell_type": "code",
      "source": [
        "# Load the dataset with spark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"JaccardSimilarity\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "books_rating = spark.read.csv(\"data/Books_rating.csv\", header=True, inferSchema=True)"
      ],
      "id": "faa012f3baac4263",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-30T14:39:09.362525Z",
          "start_time": "2025-04-30T14:39:09.312102Z"
        },
        "id": "6f55fd13c5c4a70c"
      },
      "cell_type": "code",
      "source": [
        "# Display the first few rows of the books_rating dataset\n",
        "books_rating.show(5)"
      ],
      "id": "6f55fd13c5c4a70c",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "32f883874724060"
      },
      "cell_type": "markdown",
      "source": [
        "# Create a subsample of the dataset for performance reasons\n",
        "I will use a sample of the dataset as a test case. My local and Google Collab computers are not powerful enough to handle the full dataset. I will use *sample* as a parameter can it can be changed to the desired sample size."
      ],
      "id": "32f883874724060"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-30T14:39:09.366021Z",
          "start_time": "2025-04-30T14:39:09.363520Z"
        },
        "id": "53024733b148bdb4"
      },
      "cell_type": "code",
      "source": [
        "sample = 0.01 # 1% of the dataset"
      ],
      "id": "53024733b148bdb4",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-30T14:39:10.907581Z",
          "start_time": "2025-04-30T14:39:09.367018Z"
        },
        "id": "c1caef794a13d7d1"
      },
      "cell_type": "code",
      "source": [
        "books_rating_sample = books_rating.sample(fraction=sample, seed=42)\n",
        "books_rating_sample.show(5)\n",
        "sample_size = books_rating_sample.count()"
      ],
      "id": "c1caef794a13d7d1",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sample_size"
      ],
      "metadata": {
        "id": "nP9yHAcUa9C0"
      },
      "id": "nP9yHAcUa9C0",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dcb6e82c7d972535"
      },
      "cell_type": "markdown",
      "source": [
        "# Checking for null values in the 'review/text' column\n",
        "Since we are using text data, we need to check for null values in the 'review/text' column in order to take a decision on how to handle them."
      ],
      "id": "dcb6e82c7d972535"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-30T14:39:17.415112Z",
          "start_time": "2025-04-30T14:39:10.907581Z"
        },
        "id": "dcf56755a2c3a6d5"
      },
      "cell_type": "code",
      "source": [
        "# Check for null values in the 'review/text' column\n",
        "null_rows_df = books_rating.filter(\n",
        "    books_rating['review/text'].isNull() | (books_rating['review/text'].rlike('^\\\\s*$'))\n",
        ")\n",
        "\n",
        "null_rows_df.show(30)"
      ],
      "id": "dcf56755a2c3a6d5",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "13b84fb5ae57aa62"
      },
      "cell_type": "markdown",
      "source": [
        "Since the 'review/text' null values are more or less duplicates.\n",
        "\n",
        "They share the same columns, besides the identifiers and the Title. Although the Title is not the same, with a qualitative check we can see they are about the same book."
      ],
      "id": "13b84fb5ae57aa62"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-30T14:39:19.227590Z",
          "start_time": "2025-04-30T14:39:17.416108Z"
        },
        "id": "5885180066a66507"
      },
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "filtered_df = books_rating.filter(\n",
        "    col(\"Title\").contains(\"Lord of the Rings\")\n",
        ").select(\"Title\", \"review/text\").limit(100)\n",
        "\n",
        "# Show the result\n",
        "filtered_df.show(100, truncate=False)"
      ],
      "id": "5885180066a66507",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "6dcd6513675d7e10"
      },
      "cell_type": "markdown",
      "source": [
        "There are many others Lord of The Rings reviews. I think we can't discard the NA values and it's better to keep them. We can use also the Title as similarity check."
      ],
      "id": "6dcd6513675d7e10"
    },
    {
      "metadata": {
        "id": "a2cdb61db5f854cf"
      },
      "cell_type": "markdown",
      "source": [],
      "id": "a2cdb61db5f854cf"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-30T14:39:26.897067Z",
          "start_time": "2025-04-30T14:39:19.228584Z"
        },
        "id": "45b44dde088bb228"
      },
      "cell_type": "code",
      "source": [
        "# Check for null values in the 'review/text' column\n",
        "null_rows_df_sample = books_rating_sample.filter(\n",
        "    books_rating_sample['review/text'].isNull() | (books_rating_sample['review/text'].rlike('^\\\\s*$'))\n",
        ")\n",
        "\n",
        "null_rows_df_sample.show(30)"
      ],
      "id": "45b44dde088bb228",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "fbdccfaafc82187e"
      },
      "cell_type": "markdown",
      "source": [
        "The is a null value in the subsample"
      ],
      "id": "fbdccfaafc82187e"
    },
    {
      "metadata": {
        "id": "533cd4951d48f00d"
      },
      "cell_type": "markdown",
      "source": [
        "# Jaccard Similarity"
      ],
      "id": "533cd4951d48f00d"
    },
    {
      "metadata": {
        "id": "1072c0152b9ecab2"
      },
      "cell_type": "markdown",
      "source": [
        "## Tokenization\n",
        "Since the dataset is too big, I will use a subsample of the dataset to test the Jaccard Similarity approach, for test purposes. I will then try top scale it to the full dataset.\n",
        "In addition, I will use the MapReduce approach to parallelize the computation of the Jaccard Similarity."
      ],
      "id": "1072c0152b9ecab2"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-30T14:39:26.901660Z",
          "start_time": "2025-04-30T14:39:26.898060Z"
        },
        "id": "5885315f727b57eb"
      },
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import ArrayType, StringType"
      ],
      "id": "5885315f727b57eb",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-30T14:39:26.913561Z",
          "start_time": "2025-04-30T14:39:26.902649Z"
        },
        "id": "c6be9ffd0f620cb9"
      },
      "cell_type": "code",
      "source": [
        "tokenizer = RegexTokenizer(inputCol=\"review/text\", outputCol=\"reviews/tokens\", pattern=\"\\\\W\") # \\\\W is a regex pattern that matches any non-word character"
      ],
      "id": "c6be9ffd0f620cb9",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "d1275a324da74d22"
      },
      "cell_type": "markdown",
      "source": [
        "It tokenizes the text into words. I am trying this approach because I believe it will be faster then some neural network based approach, since this use sparks. It will have some limitations, since it does not take in account of compound words, but I think it will be good for the first approach."
      ],
      "id": "d1275a324da74d22"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-30T14:39:26.948505Z",
          "start_time": "2025-04-30T14:39:26.914558Z"
        },
        "id": "1fca258eea1e93f1"
      },
      "cell_type": "code",
      "source": [
        "stopwords_remover = StopWordsRemover(inputCol=\"reviews/tokens\", outputCol=\"tokens/nostopwords\")"
      ],
      "id": "1fca258eea1e93f1",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "87c07acc08025c2c"
      },
      "cell_type": "markdown",
      "source": [
        "StopWordsRemover() uses a predefined list of stop words in English. It removes common words that do not carry much meaning, such as \"the\", \"is\", \"in\", etc. This is important, since the Jaccard Similarity is based on the number of common words between two texts. If we don't remove the stop words, the Jaccard Similarity will be biased by the number of stop words in the texts, which are not relevant for the meaning of the texts."
      ],
      "id": "87c07acc08025c2c"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-30T14:39:26.952603Z",
          "start_time": "2025-04-30T14:39:26.949498Z"
        },
        "id": "4707a6f993b28803"
      },
      "cell_type": "code",
      "source": [
        "def remove_numbers(tokens):\n",
        "    return [token for token in tokens if not token.isdigit()]"
      ],
      "id": "4707a6f993b28803",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "593cab392a6b24df"
      },
      "cell_type": "markdown",
      "source": [
        "Numbers are often not relevant for the meaning of a review. In our dataset, the score is stored in a separated column, so we can remove the numbers from the text."
      ],
      "id": "593cab392a6b24df"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-30T14:39:26.959855Z",
          "start_time": "2025-04-30T14:39:26.953595Z"
        },
        "id": "5a62598dcf6f25a2"
      },
      "cell_type": "code",
      "source": [
        "remove_numbers_udf = udf(remove_numbers, ArrayType(StringType())) # Specify the return type as ArrayType(StringType), meaning an array/list of strings"
      ],
      "id": "5a62598dcf6f25a2",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "c55d3aa47c162dab"
      },
      "cell_type": "markdown",
      "source": [
        "Sparks does not work as a Python object, so we need UDF (user defined function) to use it in the pipeline."
      ],
      "id": "c55d3aa47c162dab"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-30T14:39:27.037674Z",
          "start_time": "2025-04-30T14:39:26.960848Z"
        },
        "id": "ddaa3e8ad4c3282"
      },
      "cell_type": "code",
      "source": [
        "tokenized_books_rating_sample = tokenizer.transform(books_rating_sample) # Apply the tokenizer to the dataset\n",
        "tokenized_reviews_nostopwords_sample = stopwords_remover.transform(tokenized_books_rating_sample) # Remove the stop words from the tokenized dataset\n",
        "tokenized_rw_nsw_nn_sample = tokenized_reviews_nostopwords_sample.withColumn(\"tokens/nostopwords/nonumbers\", remove_numbers_udf(col(\"tokens/nostopwords\"))) # Remove the numbers from the dataset"
      ],
      "id": "ddaa3e8ad4c3282",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-30T14:39:27.041769Z",
          "start_time": "2025-04-30T14:39:27.038669Z"
        },
        "id": "ebbb5d0b219c3eb9"
      },
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(\"Python Version:\", sys.version)  # Check if it prints Python 3.12 in both worker and driver."
      ],
      "id": "ebbb5d0b219c3eb9",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-04-30T14:39:28.242467Z",
          "start_time": "2025-04-30T14:39:27.042762Z"
        },
        "id": "337bce825f5607c6"
      },
      "cell_type": "code",
      "source": [
        "# Verify that the environment variable is set correctly\n",
        "\n",
        "books_rating_sample.show(10)\n",
        "tokenized_rw_nsw_nn_sample.show(10)"
      ],
      "id": "337bce825f5607c6",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import CountVectorizer\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_safe = tokenized_rw_nsw_nn_sample \\\n",
        "    .filter(col(\"review/text\").isNotNull())\n",
        "\n",
        "cv = CountVectorizer(\n",
        "    inputCol=\"tokens/nostopwords/nonumbers\",  # No renaming here\n",
        "    outputCol=\"features\",\n",
        "    vocabSize=5000,\n",
        "    minDF=10\n",
        ")\n",
        "\n",
        "model = cv.fit(df_safe)\n",
        "vocab = model.vocabulary\n"
      ],
      "metadata": {
        "id": "q-0wCeXlSEcl"
      },
      "id": "q-0wCeXlSEcl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top30 = model.vocabulary[:30]\n",
        "print(\"Top 30 tokens (approx. by frequency):\")\n",
        "for i, token in enumerate(top30, start=1):\n",
        "    print(f\"{i:2d}. {token}\")"
      ],
      "metadata": {
        "id": "BGbA8ZIMXG4B"
      },
      "id": "BGbA8ZIMXG4B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method approximates the top 30 words in the vocaboulary. However, it doesn't show the frequency, since CountVectorizer() only approximates the real frequency of the top words. I want to find them."
      ],
      "metadata": {
        "id": "juJWNAF1ZBWJ"
      },
      "id": "juJWNAF1ZBWJ"
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode, col\n",
        "\n",
        "# 1) Explode the original tokens column into one token per row\n",
        "exploded = df_safe.select(explode(col(\"tokens/nostopwords/nonumbers\")).alias(\"token\"))\n",
        "\n",
        "# 2) Filter to only the top-K tokens\n",
        "topK_set = set(model.vocabulary[:20])  # assumes you want top 20 from vocab\n",
        "filtered = exploded.filter(col(\"token\").isin(topK_set))\n",
        "\n",
        "# 3) Group and count\n",
        "exact_counts = (\n",
        "    filtered\n",
        "      .groupBy(\"token\")\n",
        "      .count()\n",
        "      .orderBy(col(\"count\").desc())\n",
        ")\n",
        "\n",
        "exact_counts.show(truncate=False)"
      ],
      "metadata": {
        "id": "nm1ZhXjZZRlD"
      },
      "id": "nm1ZhXjZZRlD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1OgbY8RXbRWt"
      },
      "id": "1OgbY8RXbRWt"
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 0.20 * sample_size\n",
        "print(threshold)"
      ],
      "metadata": {
        "id": "re8lQKJXcrg8"
      },
      "id": "re8lQKJXcrg8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I defined a function that only keeps the tokens which are not in the custom_stopwords list"
      ],
      "metadata": {
        "id": "UUpgocTXcsgH"
      },
      "id": "UUpgocTXcsgH"
    },
    {
      "cell_type": "code",
      "source": [
        "high_freq_tokens = (\n",
        "    exact_counts\n",
        "      .filter(col(\"count\") > threshold)\n",
        "      .select(\"token\")\n",
        "      .rdd.flatMap(lambda x: x)\n",
        "      .collect()\n",
        ")"
      ],
      "metadata": {
        "id": "QE1X2OXzcAw8"
      },
      "id": "QE1X2OXzcAw8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "high_freq_tokens"
      ],
      "metadata": {
        "id": "X8UmpBKreiMU"
      },
      "id": "X8UmpBKreiMU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a UDF to filter them out\n",
        "def remove_custom_stopwords(tokens):\n",
        "    return [t for t in tokens if t not in high_freq_tokens]\n",
        "\n",
        "remove_udf = udf(remove_custom_stopwords, ArrayType(StringType()))\n",
        "\n",
        "tokenized_sample_2 = tokenized_rw_nsw_nn_sample.withColumn(\n",
        "    \"tokens_clean\",\n",
        "    remove_udf(col(\"tokens/nostopwords/nonumbers\"))\n",
        ")"
      ],
      "metadata": {
        "id": "T3qIjrVVeBYW"
      },
      "id": "T3qIjrVVeBYW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_safe = tokenized_sample_2.filter(col(\"review/text\").isNotNull())\n",
        "\n",
        "cv = CountVectorizer(\n",
        "    inputCol=\"tokens_clean\",\n",
        "    outputCol=\"features\",\n",
        "    vocabSize=5000,\n",
        "    minDF=10\n",
        ")\n",
        "\n",
        "model2 = cv.fit(df_safe)\n",
        "vocab2 = model2.vocabulary\n"
      ],
      "metadata": {
        "id": "ljhJMlsIfHxo"
      },
      "id": "ljhJMlsIfHxo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top30 = model2.vocabulary[:30]\n",
        "print(\"Top 30 tokens (approx. by frequency):\")\n",
        "for i, token in enumerate(top30, start=1):\n",
        "    print(f\"{i:2d}. {token}\")"
      ],
      "metadata": {
        "id": "Cm9d9gHdfNSO"
      },
      "id": "Cm9d9gHdfNSO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local Sensitive Hashing\n",
        "\n",
        "Since the project must be scalable, we can't compute the Jaccard similarity on the whole dataset. Instead, we will\n"
      ],
      "metadata": {
        "id": "Nq5P2ClqBkx2"
      },
      "id": "Nq5P2ClqBkx2"
    },
    {
      "cell_type": "code",
      "source": [
        "sim_threshold = 0.6"
      ],
      "metadata": {
        "id": "yoUalgkRVszv"
      },
      "id": "yoUalgkRVszv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_featurized = model2.transform(df_safe)"
      ],
      "metadata": {
        "id": "BP8Ji5boMcMt"
      },
      "id": "BP8Ji5boMcMt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Hash Function doesn't work with strings, it need vectors"
      ],
      "metadata": {
        "id": "FgUbUAFwXFoI"
      },
      "id": "FgUbUAFwXFoI"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_featurized.show(10)"
      ],
      "metadata": {
        "id": "QLA_EoRRUDtq"
      },
      "id": "QLA_EoRRUDtq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import MinHashLSH\n",
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "mh = MinHashLSH(\n",
        "    inputCol=\"features\",\n",
        "    outputCol=\"hashes\",\n",
        "    numHashTables=10\n",
        ")\n",
        "mhModel = mh.fit(tokenized_featurized)\n",
        "hashed = mhModel.transform(tokenized_featurized)"
      ],
      "metadata": {
        "id": "Dg9NdEyYUlbP"
      },
      "id": "Dg9NdEyYUlbP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Map tokens to a digit and select the min among the hash functions"
      ],
      "metadata": {
        "id": "gSb00eDTXLUh"
      },
      "id": "gSb00eDTXLUh"
    },
    {
      "cell_type": "code",
      "source": [
        "candidates = mhModel.approxSimilarityJoin(\n",
        "    hashed, hashed,\n",
        "    threshold=1.0 - sim_threshold,\n",
        "    distCol=\"jaccardDist\"\n",
        ").filter(col(\"datasetA.Id\") < col(\"datasetB.Id\")) # Filters out redundant pairs\n",
        "\n",
        "results_pandas = candidates.toPandas()"
      ],
      "metadata": {
        "id": "pceNiTkiWAke"
      },
      "id": "pceNiTkiWAke",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Self join and then compute the Jaccard Similarity for each option and filters out pairs under the threshold."
      ],
      "metadata": {
        "id": "iGHH3ExaX9mB"
      },
      "id": "iGHH3ExaX9mB"
    },
    {
      "cell_type": "code",
      "source": [
        "results = candidates.select(\n",
        "        col(\"datasetA.Id\").alias(\"idA\"),\n",
        "        col(\"datasetB.Id\").alias(\"idB\"),\n",
        "        (1.0 - col(\"jaccardDist\")).alias(\"approxJaccard\")\n",
        "    )"
      ],
      "metadata": {
        "id": "kLL0HyuPXkZJ"
      },
      "id": "kLL0HyuPXkZJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VI_8iw-SZ2I3"
      },
      "id": "VI_8iw-SZ2I3"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (python312bigdata)",
      "language": "python",
      "name": "python312bigdata"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}